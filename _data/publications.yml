- title: Jointly measuring diversity and quality in text generation models
  link: https://www.aclweb.org/anthology/W19-2311.pdf
  pdf: https://www.aclweb.org/anthology/W19-2311.pdf
  github: IAmS4n/TextGenerationEvaluationMetrics
  slide: https://docs.google.com/presentation/d/1S-kgqCYNeC9SiIOQ_GshQvVnJYEp1yXARcZAe-a2oDg
  youtube: x0MDJe4Oc4k
  poster: /posters/NAACL2019_NeuralGen_JointlyMeasuring.pdf
  details: |
    **Montahaei, E.**, Alihosseini, D., & Baghshah, M. S. (2019, In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation.).
  short_abstract: |
    Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity.
  abstract: | # this will include new lines to allow paragraphs
    Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generator's density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.

- title: Adversarial classifier for imbalanced problems
  link: https://arxiv.org/abs/1811.08812
  pdf: https://arxiv.org/pdf/1811.08812.pdf
  details: |
    **Montahaei, E.**, Ghorbani, M., Baghshah, M. S., & Rabiee, H. R. (2018)
  short_abstract: |
    Adversarial approach has been widely used for data generation in the last few years. However, this approach has not been extensively utilized for classifier training. In this paper, we propose an adversarial framework for classifier training that can also handle imbalanced data.
  abstract: |
    Adversarial approach has been widely used for data generation in the last few years. However, this approach has not been extensively utilized for classifier training. In this paper, we propose an adversarial framework for classifier training that can also handle imbalanced data. Indeed, a network is trained via an adversarial approach to give weights to samples of the majority class such that the obtained classification problem becomes more challenging for the discriminator and thus boosts its classification capability. In addition to the general imbalanced classification problems, the proposed method can also be used for problems such as graph representation learning in which it is desired to discriminate similar nodes from dissimilar nodes. Experimental results on imbalanced data classification and on the tasks like graph link prediction show the superiority of the proposed method compared to the state-of-the-art methods.

- title: Efficient continuous skyline computation on multi-core processors based on Manhattan distance
  link: https://ieeexplore.ieee.org/abstract/document/7340469
  pdf: /papers/EfficientSkylineManhattan.pdf
  slide: /slide/EfficientSkylineManhattan.pdf
  details: |
    **Montahaei, E.**, Ghafouri, M., Rahmani, S., Ghasemi, H., Bakhtiar, F. S., Zamanshoar, R., ... & Gorgin, S. (2015, In 2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign. IEEE.)
  short_abstract: |
    The continuous Skyline query has recently become the subject of the several researches due to its wide spectrum of applications such as multi-criteria decision making, graph analysis network, wireless sensor network and data exploration.
  abstract: |
    The continuous Skyline query has recently become the subject of the several researches due to its wide spectrum of applications such as multi-criteria decision making, graph analysis network, wireless sensor network and data exploration. In these applications, the datasets are huge and have various dimensions. Moreover, they constantly change as time passes. Therefore, this query is considered as a computation intensive operation that finding the result in a reasonable time is a challenge. In this paper, we present an efficient parallel continuous Skyline approach. In our suggested method, the dataset points are sorted and pruned based on Manhattan distance. Moreover, we use several optimization methods to optimize memory usage in comparison with na√Øve implementation. In addition, besides the applied conventional parallelization methods, we partition the time steps based on the number of available cores. The experimental results for a dataset that contains 800k points with 7 dimensions show considerable speedup.